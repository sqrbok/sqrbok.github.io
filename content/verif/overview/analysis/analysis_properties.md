---
parent: Analysis
nav_order: 2
title: Software Properties
layout: default
---

## ğŸ” Software Properties Verified by Static Analysis

**Definition:**  
Safety properties ensure thatÂ _nothing bad happens_Â during program execution. These properties guarantee the system doesÂ **not reach an undesirable state**.

**Examples:**

- NoÂ **deadlocks**Â (e.g. multiple processes waiting forever for each other).
    
- No attempt toÂ **access an empty buffer**Â (e.g. removing an item that isnâ€™t there).
    

**Purpose:**  
Used to ensure theÂ **consistency of program state**.

**How?**

- EnforceÂ **mutual exclusion**: Shared resources must be accessed atomically (only one thread/process at a time).
    
- UseÂ **condition synchronization**: Certain actions are delayed until the system is in a safe state.
    

---

### âœ… Liveness Properties

**Definition:**  
Liveness properties ensure thatÂ _something good eventually happens_. They are used to guaranteeÂ **progress**Â in the system.

**Prevents:**

- **Starvation**: A process never gets needed resources (like CPU time).
    
- **Dormancy**: A waiting process is never resumed.
    
- **Premature termination**: A process ends before it should.
    

---

### âš–ï¸ Fairness Properties

**Definition:**  
A subset of liveness properties.Â **Fairness**Â means that every process or action gets its fair shareÂ _infinitely often_.

**Example:**

- A process is guaranteed to be activated repeatedly (e.g. each thread gets a turn in a scheduler).
    

---

### â³ Temporal Properties

**Definition:**  
Temporal properties describe how states are relatedÂ **over time**. UnlikeÂ **state properties**Â (which describe a single point),Â **temporal properties**Â describeÂ **paths or sequences of states**.

**Example:**  
â€œIf a message is sent in one state, it will eventually be received in a future state.â€

**Notation (common temporal logic):**

- **Î±**: Î± holds in the current state
    
- **XÎ±**: Î± holds in theÂ _next_Â state
    
- **FÎ³**: Î³ holdsÂ _eventually_Â (in the future)
    
- **GÎ»**: Î» holdsÂ _always_Â from now on
    
- **Î± U Î²**: Î± holdsÂ _until_Â Î² becomes true
    

---

## ğŸ§° Analysis Fault Taxonomy (Common Issues Detected by Static Analysis)

### ğŸ§µ Concurrency Errors

- **Race conditions**: Multiple threads access shared data unsafely.
    
- **Deadlock**: Processes wait on each other in a cycle.
    
- **Improper lock usage**
    

### âš ï¸ Exceptional Conditions

- Integer overflow/underflow
    
- Division by zero
    
- Unhandled exceptions
    
- Incorrect type conversions
    

### ğŸ›¡ï¸ Input Validation Issues

- **Command injection**
    
- **Cross-site scripting (XSS)**
    
- **Format string vulnerabilities**
    
- Use ofÂ **tainted data**Â (untrusted user input)
    

### ğŸ§¹ Code Quality Issues

- PoorÂ **code metrics**
    
- **Unused variables**
    

### ğŸ’¾ Memory Errors

- **Buffer overruns**
    
- **Null or invalid pointer dereference**
    
- **Double free**Â or freeing unallocated memory
    
- **Memory leaks**
    
- Use ofÂ **uninitialized variables**
    

### ğŸ”„ Resource/Protocol Misuse

- Incorrect function call order
    
- Forgetting to initialize or free resources
    

### ğŸ§  Design & Structural Issues

- ComplexÂ **dependencies**
    
- ComplicatedÂ **heap structures**
    
- Incomplete or messyÂ **call graphs**
    

### ğŸ” Security Weaknesses

- **Privilege escalation**
    
- **Denial of service (DoS)**
    
- Execution ofÂ **dynamic code**
    
- **Insecure randomness**
    
- ViolatingÂ **least privilege principle**
    

---

## ğŸ§ª How does the Analysis work?

**Software analysis**Â involves creating aÂ **model**Â of the system (either manually or automatically), and thenÂ **verifying properties**Â of the system using the modelâ€”**without running**Â the actual software.

> By abstracting away unnecessary details, we can prove or disprove whether certain properties hold.

---

## ğŸ¯ Precision of Static Analysis

- **Soundness**:  
    If the tool says the program isÂ **correct**, it truly is.  
    âœ… No false negatives (i.e. no missed bugs).
    
- **Completeness**:  
    If the tool reports anÂ **issue**, itâ€™s real.  
    âœ… No false positives (i.e. no bogus warnings).
    

> â— In practice,Â **no static analysis can be both sound and complete**Â and stillÂ **terminate**Â on all programs.

### Reality of Static Analysis:

- **Perfect static analysis is undecidable**: itâ€™s impossible to create one that works perfectly for all programs.
    
- All analysesÂ **use abstraction**Â to approximate behavior.
    
- Still, static analysis tools are extremelyÂ **useful in practice**.
    

---

# Soundness and Completeness in Static Analysis

## What is Soundness?

- AÂ **sound**Â static analysisÂ _over-approximates_Â the behaviors of a program.
    
- This means itÂ **guarantees to find all violations**Â of a given property â€” if a bug exists, the tool will catch it.
    
- However, because it is conservative, it may reportÂ **false alarms**Â â€” warnings about issues thatÂ **cannot actually happen**Â in the real program.
    

**In short:**  
Sound analysis â†’Â _No real bugs missed_Â but may haveÂ _false positives_.

---

## What is Completeness?

- AÂ **complete**Â static analysisÂ _under-approximates_Â the program's behavior.
    
- This means thatÂ **every violation it reports is real**Â â€” no false alarms.
    
- But itÂ **might miss some real bugs**Â because it only reports violations it can confidently confirm.
    

**In short:**  
Complete analysis â†’Â _All reported bugs are real_Â but mayÂ _miss some bugs_.

---

## Why Can't We Have Both Soundness and Completeness?

- According toÂ **Riceâ€™s Theorem**, there are fundamental limits on what automated program analysis can achieve.
    
- It is impossible for a static analysis to beÂ **sound, complete, and always terminate**Â on all programs with arbitrary complexity (such as unbounded memory structures).
    

In other words, no tool can:

- Miss no errors (sound)
    
- Have no false alarms (complete)
    
- Automatically analyze every program and always finish (terminating)
    

---

## Practical Implications

- **For high assurance (critical systems), soundness is vital:**  
    If a tool saysÂ _no errors_, then there really are none. This means we accept some false alarms.
    
- **For general software development, bug finding is the main goal:**  
    Most tools prioritize reducing the number of bugs over guaranteeing to find all bugs.  
    Users often cannot tolerate many false alarms, so tools sacrifice soundness to be more practical.
    
- Some tools tradeÂ **automation for accuracy**Â by requiringÂ **user annotations**Â or guidance.
    

---

## Examples of Commercial Tools

- Tools likeÂ **Coverity**,Â **CodeSonar**,Â **Fortify**,Â **KlocWork**, andÂ **LDRA**Â areÂ **neither sound nor complete**.
    
- Despite this, they areÂ **effective in practice**Â at finding real bugs.
    
- They each use different methods and balance between false alarms and missed bugs differently.
    

---

## What About Simpler Tools Like Lint and FindBugs?

- These tools useÂ **pattern matching**Â rather than deep semantic analysis.
    
- They areÂ **neither sound nor complete**, but they are still useful and widely used.
    
- They help catch common bugs quickly and easily.
    

---

### Summary Table

|Property|What it Means|Pros|Cons|
|---|---|---|---|
|**Sound**|No bugs missed (no false negatives)|High assurance|May report false alarms (false positives)|
|**Complete**|No false alarms (no false positives)|Accurate warnings|May miss real bugs|
|**Neither**|Mix of both|Practical and efficient|Some bugs missed, some false alarms|


# Limitations of Static Analysis and Formal Verification

Static analysis and related verification methods are powerful tools for improving software quality, but they come with inherent limitations. Understanding these limitations helps set realistic expectations about what these tools can and cannot do.

---

## 1. Non-Functional Verification

- **What it means:**  
    Most static analysis tools focus on verifyingÂ _functional correctness_Â (e.g., no null-pointer dereferences, race conditions, or deadlocks). However, many importantÂ _non-functional_Â properties are not easily verified, such as:
    
    - Performance (e.g., execution speed, responsiveness)
        
    - Memory usage or power consumption
        
    - Usability or maintainability
        
    - Security properties that depend on runtime environment or configuration
        
- **Example:**  
    A static analyzer may verify that a program never accesses out-of-bounds memory but cannot guarantee that it will run efficiently under all circumstances.
    
- **Implication:**  
    Non-functional properties often require other tools or runtime monitoring.
    

---

## 2. False Positives (Spurious Warnings)

- **What it means:**  
    AÂ **false positive**Â occurs when the tool reports a bug or violation thatÂ _does not actually exist_Â in any real execution.
    
- **Why it happens:**  
    To ensure soundness (catching all real bugs), tools often over-approximate program behavior, which can cause them to flag safe operations as risky.
    
- **Example:**  
    A static analyzer might warn about a potential null pointer dereference on a variable that is, in fact, guaranteed to be initialized correctly, but the tool cannot prove it precisely.
    
- **Impact on developers:**  
    False positives can cause "alert fatigue," making developers ignore or disable the tool, reducing its effectiveness.
    

---

## 3. False Negatives (Missed Bugs)

- **What it means:**  
    AÂ **false negative**Â occurs when the tool fails to detect a real bug or violation.
    
- **Why it happens:**  
    Tools that aim for completeness or to reduce false positives often under-approximate behaviors and thus may miss some bugs.
    
- **Example:**  
    A heuristic-based security scanner might fail to detect a subtle injection vulnerability because it cannot analyze all code paths precisely.
    
- **Risk:**  
    False negatives reduce confidence in the tool and might allow critical bugs to remain undetected.
    

---

## 4. Performance and Scalability

- **What it means:**  
    Static analysis, especially exhaustive techniques like model checking or formal proofs, can be very computationally expensive.
    
- **Why it happens:**  
    Analyzing all possible execution paths, states, or program behaviors grows exponentially with program size (state explosion problem).
    
- **Example:**  
    Model checkers like SPIN or Java PathFinder might run out of memory or time when analyzing large, complex programs with many concurrent threads.
    
- **Trade-off:**  
    Tools often limit the depth of analysis or abstract details to improve performance, trading off completeness or soundness.
    

---

## 5. Trade-offs Between Soundness, Completeness, and Automation

- The ideal tool would be:
    
    - **Sound**: never miss real bugs
        
    - **Complete**: never report false bugs
        
    - **Fully automatic**: require no human guidance
        
- **Riceâ€™s theorem and related results**Â show this is impossible for all non-trivial properties in programs.
    
- **Thus, practical tools must compromise**, choosing which properties to check, which false alarms to tolerate, or which user inputs to require.
    

---

## Summary

|Limitation|Explanation|Example|Impact|
|---|---|---|---|
|Non-functional|Can't verify performance, usability, etc.|Can't guarantee program runs fast enough|May need runtime profiling|
|False Positives|Warns about issues that aren't real|Warns on safe null pointer usage|Can overwhelm developers|
|False Negatives|Misses real bugs|Misses subtle security flaw|Bugs slip into production|
|Performance/Scalability|Analysis can be very slow or impossible on large code|Model checking runs out of memory/time|Limits tool use on large projects|

---

## Real-World Examples

- **Coverity**Â andÂ **Fortify**Â balance false positives and negatives by configurable heuristics and human-in-the-loop review.
    
- **SPIN model checker**Â is powerful but may not scale well for very large or highly concurrent systems without abstraction.
    
- **Lint and FindBugs**Â are fast and scalable but often produce many false positives due to their pattern-based approach.
    

---

## Final Note

Despite limitations, static analysis and formal verification remain critical in improving software reliability and security. Combining multiple tools and techniques â€” static analysis, dynamic testing, code reviews, and runtime monitoring â€” provides the best overall assurance.

---


---

{: .highlight }
**Disclaimer:** AI is used for text polishing and explaining. Authors have verified all facts and claims. In case of an error, feel free to file an issue.
